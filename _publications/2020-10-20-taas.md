---
title: "Topic-Aware Abstractive Text Summarization"
collection: publications
permalink: /publications/2020-10-20-taas
date: 2020-10-20
paperurl: 'https://arxiv.org/pdf/2010.10323.pdf'
citation: 'Zheng, C., Zhang, K., Wang, H. J.,&amp; Fan, L. (2020, October 20). Topic-Aware Abstractive Text Summarization. Retrieved October 20, 2020, from https://arxiv.org/pdf/2010.10323'
---
Abstract:

Automatic text summarization aims at condensing a document to a shorter version while preserving the key information. Differ- ent from extractive summarization which simply selects text frag- ments from the document, abstractive summarization generates the summary in a word-by-word manner. Most current state-of- the-art (SOTA) abstractive summarization methods are based on the Transformer-based encoder-decoder architecture and focus on novel self-supervised objectives in pre-training. While these models well capture the contextual information among words in documents, little attention has been paid to incorporating global semantics to better fine-tune for the downstream abstractive summarization task.

In this study, we propose a topic-aware abstractive summariza- tion (TAAS) framework by leveraging the underlying semantic structure of documents represented by their latent topics. Specifi- cally, TAAS seamlessly incorporates a neural topic modeling into an encoder-decoder based sequence generation procedure via at- tention for summarization. This design is able to learn and preserve global semantics of documents and thus makes summarization ef- fective, which has been proved by our experiments on real-world datasets. As compared to several cutting-edge baseline methods, we show that TAAS outperforms BART, a well-recognized SOTA model, by 2%, 8%, and 12% regarding the F measure of ROUGE-1, ROUGE-2, and ROUGE-L, respectively. TAAS also achieves compa- rable performance to PEGASUS and ProphetNet, which is difficult to accomplish given that training PEGASUS and ProphetNet re- quires enormous computing capacity beyond what we used in this study.

[Download paper here](https://arxiv.org/pdf/2010.10323.pdf)